import os
import json
import uuid
from typing import List, Dict
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from dotenv import load_dotenv, find_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())
OpenAI_API_KEY = os.getenv("OPENAI_API_KEY")
OpenAI_API_BASE = os.getenv("OPENAI_API_BASE")
# --- é…ç½® ---
TARGET_FOLDER = "D:/a_job/y1/project+train/llm_learn/my_rag/documents/datas/md"  # ä½ çš„Markdownæ–‡ä»¶å¤¹è·¯å¾„
OUTPUT_FILE = "agent_knowledge_base.json" # è¾“å‡ºæ–‡ä»¶å

# --- å®šä¹‰è¾“å‡ºæ ¼å¼ ---
class QAPair(BaseModel):
    question: str = Field(description="åŸºäºæ–‡æœ¬ç”Ÿæˆçš„å…·ä½“é—®é¢˜")
    answer: str = Field(description="åŸºäºæ–‡æœ¬çš„è¯¦ç»†ç­”æ¡ˆ")
    keywords: List[str] = Field(description="æ¶‰åŠçš„æ ¸å¿ƒå®ä½“æˆ–å…³é”®è¯")

# --- æ ¸å¿ƒé€»è¾‘ ---
def process_markdown_folder(folder_path: str) -> List[Dict]:
    """éå†æ–‡ä»¶å¤¹å¹¶åŠ è½½MDæ–‡ä»¶"""
    all_chunks = []
    
    # 1. å®šä¹‰Markdownåˆ†å‰²ç­–ç•¥ (æŒ‰æ ‡é¢˜åˆ‡åˆ†ä»¥ä¿ç•™è¯­ä¹‰å®Œæ•´æ€§)
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    # äºŒæ¬¡åˆ‡åˆ†ï¼šé˜²æ­¢æŸä¸ªç« èŠ‚è¿‡é•¿ï¼Œè¶…è¿‡LLMçª—å£æˆ–æ£€ç´¢å—é™åˆ¶
    char_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)

    print(f"ğŸ“‚ å¼€å§‹æ‰«ææ–‡ä»¶å¤¹: {folder_path} ...")

    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                print(f"   å¤„ç†æ–‡ä»¶: {file}")
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    # ç¬¬ä¸€æ¬¡åˆ‡åˆ†ï¼šæŒ‰ç« èŠ‚
                    header_splits = md_splitter.split_text(text)
                    
                    # ç¬¬äºŒæ¬¡åˆ‡åˆ†ï¼šæŒ‰é•¿åº¦
                    final_splits = char_splitter.split_documents(header_splits)
                    
                    for split in final_splits:
                        # è¡¥å……å…ƒæ•°æ®
                        split.metadata["source_file"] = file
                        all_chunks.append(split)
                        
                except Exception as e:
                    print(f"   âŒ è¯»å–å¤±è´¥ {file}: {e}")

    print(f"âœ… å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œå¼€å§‹ç”Ÿæˆ QA...")
    return all_chunks
def generate_qa_pairs(chunks: List, output_path: str):
    """åˆ©ç”¨ LLM ä¸ºæ¯ä¸ªç‰‡æ®µç”Ÿæˆ QA"""
    
    # --- ä¿®æ­£éƒ¨åˆ† Start ---
    # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å: openai_api_key å’Œ openai_api_base
    llm = ChatOpenAI(
        model="gpt-4o-mini", 
        temperature=0.5, 
        openai_api_key=OpenAI_API_KEY, 
        openai_api_base=OpenAI_API_BASE
    ) 
    # --- ä¿®æ­£éƒ¨åˆ† End ---
    
    parser = JsonOutputParser(pydantic_object=QAPair)
    
    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸º RAG æ™ºèƒ½ä½“æ„å»ºçŸ¥è¯†åº“çš„ä¸“å®¶ã€‚
        è¯·é˜…è¯»ä¸‹é¢çš„æŠ€æœ¯æ–‡æ¡£ç‰‡æ®µï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚
        
        æ–‡æ¡£ç‰‡æ®µ:
        {context}
        
        è¦æ±‚:
        1. **é—®é¢˜ (Question)**: å¿…é¡»æ˜¯ç”¨æˆ·å¯èƒ½ä¼šé—®çš„çœŸå®é—®é¢˜ï¼ŒåŒ…å«å…·ä½“çš„å®ä½“åç§°ï¼ˆä¸è¦é—®â€œå®ƒæ˜¯ä»€ä¹ˆâ€ï¼Œè¦é—®â€œTx-Moduleæ˜¯ä»€ä¹ˆâ€ï¼‰ã€‚
        2. **ç­”æ¡ˆ (Answer)**: å¿…é¡»å®Œå…¨åŸºäºæ–‡æ¡£ç‰‡æ®µï¼Œäº‹å®å‡†ç¡®ï¼Œä¸è¦ç¼–é€ ã€‚
        3. **æ ¼å¼**: å¿…é¡»æ˜¯åˆæ³•çš„ JSONã€‚
        
        {format_instructions}
        """
    )
    
    chain = prompt | llm | parser
    
    knowledge_base = []
    
    for i, chunk in enumerate(chunks):
        content = chunk.page_content
        # ç®€å•çš„è¿‡æ»¤ï¼šå¤ªçŸ­çš„ç‰‡æ®µä¸ç”Ÿæˆ
        if len(content) < 50:
            continue
            
        print(f"   [{i+1}/{len(chunks)}] ç”Ÿæˆä¸­...")
        
        try:
            # æ„é€ ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ ‡é¢˜ä¿¡æ¯å¢å¼ºè¯­ä¹‰
            header_context = " > ".join([v for k,v in chunk.metadata.items() if "Header" in k])
            full_context = f"ç« èŠ‚è·¯å¾„: {header_context}\nå†…å®¹: {content}"
            
            result = chain.invoke({
                "context": full_context,
                "format_instructions": parser.get_format_instructions()
            })
            
            # æ„å»ºæœ€ç»ˆå­˜å‚¨ç»“æ„
            entry = {
                "id": str(uuid.uuid4()),
                "source_file": chunk.metadata.get("source_file"),
                "context": full_context, 
                "question": result["question"], 
                "answer": result["answer"], 
                "keywords": result.get("keywords", [])
            }
            knowledge_base.append(entry)
            
        except Exception as e:
            # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ï¼Œæ–¹ä¾¿æ’æŸ¥
            print(f"   âš ï¸ ç”Ÿæˆå¤±è´¥ (Chunk {i}): {e}")

    # ä¿å­˜æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {output_path}")
    print(f"   å…±ç”Ÿæˆ {len(knowledge_base)} æ¡ QA æ•°æ®ã€‚")
    
if __name__ == "__main__":
    if not os.path.exists(TARGET_FOLDER):
        os.makedirs(TARGET_FOLDER)
        print(f"è¯·åœ¨ {TARGET_FOLDER} ä¸‹æ”¾å…¥ .md æ–‡ä»¶")
    else:
        chunks = process_markdown_folder(TARGET_FOLDER)
        generate_qa_pairs(chunks, OUTPUT_FILE)