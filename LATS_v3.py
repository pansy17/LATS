import os
import math
import ast
import operator
import re
from typing import List, TypedDict, Optional, Dict, Any, Union

# ----------------- ä¾èµ–åº“å¯¼å…¥ -----------------
# LangChain/LangGraph: ç”¨äºæ„å»º Agent å·¥ä½œæµå’Œå¤„ç†æ–‡æ¡£
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper
from langgraph.graph import StateGraph, END

# ----------------- ç³»ç»Ÿä¸ç¯å¢ƒé…ç½® -----------------
from dotenv import load_dotenv, find_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ API Keyï¼Œç¡®ä¿å®‰å…¨æ€§
load_dotenv(find_dotenv())
openai_api_key = os.environ.get("OPENAI_API_KEY")
openai_api_base = os.environ.get("OPENAI_API_BASE")
serper_api_key = os.environ.get("SERPER_API_KEY")

# ----------------- ç”¨æˆ·è‡ªå®šä¹‰å·¥å…·å¯¼å…¥ -----------------
# å°è¯•åŠ è½½æœ¬åœ°å®šä¹‰çš„æ··åˆæ£€ç´¢å™¨ (Ensemble Retriever)
# å¦‚æœæœ¬åœ°ç¯å¢ƒç¼ºå¤±è¯¥æ–‡ä»¶ï¼Œä¸ºäº†ä¿è¯ä»£ç å¯è¿è¡Œï¼Œå®šä¹‰ä¸€ä¸ª Mock (æ¨¡æ‹Ÿ) æ£€ç´¢å™¨
try:
    from documents.retriever_tools import ensemble_retriever
    print("âœ… æˆåŠŸåŠ è½½æœ¬åœ° RAG æ£€ç´¢å™¨: ensemble_retriever")
except ImportError:
    print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ£€ç´¢å™¨ï¼Œä½¿ç”¨ Mock æ›¿ä»£")
    class MockRetriever:
        def invoke(self, query):
            # æ¨¡æ‹Ÿè¿”å›ä¸€ä¸ªåŒ…å«å…ƒæ•°æ®çš„ Document å¯¹è±¡
            return [Document(page_content=f"æ¨¡æ‹Ÿæ£€ç´¢å†…å®¹: å…³äº {query} çš„æœ¬åœ°æŠ€æœ¯æ–‡æ¡£æ•°æ®...", metadata={"source": "local"})]
    ensemble_retriever = MockRetriever()

# ----------------- å…¨å±€æ¨¡å‹ä¸å·¥å…·åˆå§‹åŒ– -----------------
try:
    # åˆå§‹åŒ– LLM
    # temperature=0.7: åœ¨åˆ›é€ æ€§(æ‰©å±•å­æŸ¥è¯¢)å’Œä¸¥è°¨æ€§ä¹‹é—´çš„ä¸€èˆ¬å¹³è¡¡
    # åç»­åœ¨ç‰¹å®šèŠ‚ç‚¹(å¦‚ Simulation)ä¼šåŠ¨æ€è°ƒæ•´ temp ä¸º 0 ä»¥ä¿è¯äº‹å®æ€§
    llm = ChatOpenAI(
        model="gpt-4o-mini", 
        temperature=0.7, 
        openai_api_key=openai_api_key, 
        openai_api_base=openai_api_base
    )
    
    # åˆå§‹åŒ– Google æœç´¢å·¥å…· (ç”¨äºè¡¥å……å¤–éƒ¨çŸ¥è¯†)
    web_search_tool = GoogleSerperAPIWrapper(api_key=serper_api_key)
    print("âœ… æˆåŠŸåˆå§‹åŒ– Google Serper å·¥å…·")
    
except Exception as e:
    print(f"âŒ å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
    exit(1)


# ==========================================
# 1. MCTS æ ¸å¿ƒæ•°æ®ç»“æ„ (Node & State)
# ==========================================

class MCTSNode:
    """
    è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS) çš„èŠ‚ç‚¹ç±»ã€‚
    æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨æœç´¢æ ‘ä¸­çš„ä¸€ä¸ªçŠ¶æ€ï¼ˆä¸€ä¸ªå…·ä½“çš„å­æŸ¥è¯¢æˆ–æ€è€ƒæ­¥éª¤ï¼‰ã€‚
    """
    def __init__(self, parent=None, action_thought: str = "Root"):
        self.parent: Optional[MCTSNode] = parent  # çˆ¶èŠ‚ç‚¹æŒ‡é’ˆï¼Œç”¨äºåå‘ä¼ æ’­
        self.children: List[MCTSNode] = []        # å­èŠ‚ç‚¹åˆ—è¡¨
        
        # --- èŠ‚ç‚¹å†…å®¹å±æ€§ ---
        self.action_thought = action_thought  # å½“å‰èŠ‚ç‚¹çš„æ„å›¾ï¼ˆå³ç”Ÿæˆçš„å­æŸ¥è¯¢ï¼‰
        self.content: str = ""                # æ£€ç´¢å›æ¥çš„åŸå§‹æ–‡æœ¬ï¼ˆä¸Šä¸‹æ–‡ï¼‰
        self.generated_answer: str = ""       # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆçš„ä¸­é—´ç­”æ¡ˆï¼ˆå¸¦ [Ref] å¼•ç”¨ï¼‰
        
        # --- MCTS ç»Ÿè®¡å±æ€§ ---
        self.visits: int = 0                  # N: è¢«è®¿é—®/æ¨¡æ‹Ÿçš„æ¬¡æ•°
        self.value_sum: float = 0.0           # V: ç´¯è®¡è·å¾—çš„å¥–åŠ±åˆ†æ•°æ€»å’Œ
        self.depth: int = 0 if parent is None else parent.depth + 1 # æ ‘æ·±åº¦
        
    @property#è¿™ä¸ªå±æ€§ç”¨äºè·å–èŠ‚ç‚¹çš„è¯„ä¼°å€¼ï¼Œå³èŠ‚ç‚¹çš„å‡å€¼ã€‚
    def value(self) -> float:
        """è®¡ç®—å¹³å‡ä»·å€¼ Q(s, a) = V / N"""
        if self.visits == 0:
            return 0.0
        return self.value_sum / self.visits

    def uct_score(self, parent_visits: int, c_puct: float = 1.41) -> float:
        """
        è®¡ç®— UCT (Upper Confidence Bound for Trees) åˆ†æ•°ã€‚
        å…¬å¼: Q + C * sqrt(ln(N_parent) / N_child)
        ä½œç”¨: å¹³è¡¡ 'åˆ©ç”¨'(Exploitation, é€‰é«˜åˆ†èŠ‚ç‚¹) å’Œ 'æ¢ç´¢'(Exploration, é€‰å°‘è®¿é—®èŠ‚ç‚¹)
        """
        if self.visits == 0:
            return float('inf')  # æ²¡è®¿é—®è¿‡çš„èŠ‚ç‚¹ä¼˜å…ˆçº§æœ€é«˜ï¼Œä¿è¯å¹¿åº¦è¦†ç›–
        
        q_value = self.value # åˆ©ç”¨é¡¹
        # æ¢ç´¢é¡¹: è®¿é—®è¶Šå°‘ï¼Œåˆ†æ¯è¶Šå°ï¼ŒUå€¼è¶Šå¤§
        u_value = c_puct * math.sqrt(math.log(parent_visits) / (1 + self.visits))
        return q_value + u_value

    def add_child(self, child_node):
        self.children.append(child_node)

    def __repr__(self):
        # æ‰“å°èŠ‚ç‚¹æ—¶çš„è°ƒè¯•ä¿¡æ¯
        return f"<Node depth={self.depth} visits={self.visits} val={self.value:.2f} thought='{self.action_thought[:10]}...'>"

class TreeState(TypedDict):
    """
    LangGraph çš„å…¨å±€çŠ¶æ€å®šä¹‰ã€‚
    åœ¨å›¾çš„å„ä¸ªèŠ‚ç‚¹ä¹‹é—´ä¼ é€’æ•°æ®ã€‚
    """
    original_query: str     # ç”¨æˆ·æœ€åˆè¾“å…¥çš„é—®é¢˜
    normalized_query: str   # ç»è¿‡å½’ä¸€åŒ–å¤„ç†åçš„æ ‡å‡†æŸ¥è¯¢ï¼ˆå¦‚å°† stm32 è½¬ä¸º STM32ï¼‰
    root: MCTSNode          # æ•´ä¸ªæœç´¢æ ‘çš„æ ¹èŠ‚ç‚¹
    current_node: MCTSNode  # å½“å‰å·¥ä½œæµæ­£åœ¨å¤„ç†çš„èŠ‚ç‚¹æŒ‡é’ˆ
    iterations: int         # å½“å‰å·²ç»æ‰§è¡Œçš„å¾ªç¯æ¬¡æ•°
    max_iterations: int     # æœ€å¤§å…è®¸å¾ªç¯æ¬¡æ•°ï¼ˆé¢„ç®—æ§åˆ¶ï¼‰
    best_answer: str        # æœ€ç»ˆç”Ÿæˆçš„ Markdown ç ”æŠ¥

# ==========================================
# 2. LATS æ ¸å¿ƒèŠ‚ç‚¹é€»è¾‘ (Nodes)
# ==========================================

def normalize_node(state: TreeState):
    """
    ã€é¢„å¤„ç†èŠ‚ç‚¹ã€‘ï¼šæŸ¥è¯¢å½’ä¸€åŒ–
    ç›®çš„ï¼šè§£å†³åŠå¯¼ä½“é¢†åŸŸæœ¯è¯­ä¹¦å†™ä¸è§„èŒƒå¯¼è‡´çš„æ£€ç´¢æ¼å¬å›é—®é¢˜ã€‚
    ç­–ç•¥ï¼šæ­£åˆ™åŒ¹é… (è§„åˆ™) + LLM é‡å†™ (è¯­ä¹‰)ã€‚
    """
    query = state["original_query"]
    print(f"\nğŸ”§ [Normalize] æ­£åœ¨æ ‡å‡†åŒ–æŸ¥è¯¢: {query}")

    # 1. è§„åˆ™åº“å½’ä¸€åŒ– (Regex/Dict) - å¿«é€Ÿå¤„ç†å¸¸è§åˆ«å
    term_mapping = {
        r"(?i)stm\s*32": "STM32",
        r"(?i)iic": "I2C",
        r"(?i)spi": "SPI",
        r"(?i)uart": "UART",
        r"(?i)mcu": "MCU",
        r"(?i)datasheet": "Data Sheet",
        r"(?i)spec\s*sheet": "Specification",
        r"(?i)tsmc": "TSMC",
        r"(?i)smic": "SMIC",
        r"(?i)nv(idia)?": "NVIDIA",
        r"(?i)3\s*nm": "3nm",
    }
    
    normalized_query = query
    for pattern, replacement in term_mapping.items():
        normalized_query = re.sub(pattern, replacement, normalized_query)

    # 2. LLM è¯­ä¹‰é‡å†™ - å¤„ç†å¤æ‚è¯­ä¹‰å’Œå®ä½“è¡¥å…¨
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŠå¯¼ä½“é¢†åŸŸçš„æœ¯è¯­ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
    
    åŸå§‹æŸ¥è¯¢: "{normalized_query}"
    
    ä»»åŠ¡ï¼š
    1. **çº æ­£æ‹¼å†™**ï¼šä¿®å¤æ˜æ˜¾çš„æ‹¼å†™é”™è¯¯ã€‚
    2. **æœ¯è¯­è§„èŒƒ**ï¼šå°†éæ ‡å‡†æè¿°è½¬æ¢ä¸ºè¡Œä¸šé€šç”¨æœ¯è¯­ï¼ˆå¦‚ "3çº³ç±³å·¥è‰º" -> "3nm Process Node"ï¼‰ã€‚
    3. **å®ä½“è¡¥å…¨**ï¼šå¦‚æœå®ä½“åç§°æ¨¡ç³Šï¼Œå°è¯•è¡¥å…¨ï¼ˆå¦‚ "A17" -> "Apple A17 Pro"ï¼‰ï¼Œä½†ä¸è¦æ”¹å˜åŸæ„ã€‚
    4. **å»å£è¯­åŒ–**ï¼šå»é™¤æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼Œä¿ç•™æ ¸å¿ƒæœç´¢æ„å›¾ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ ‡å‡†åŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å¼•å·ã€‚
    """
    
    try:
        response = llm.invoke(prompt).content.strip()
        final_query = response.strip('"').strip("'")
    except Exception as e:
        print(f"âš ï¸ å½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ç»“æœ: {e}")
        final_query = normalized_query

    print(f"   -> æ ‡å‡†åŒ–ç»“æœ: {final_query}")
    return {"normalized_query": final_query}


def initial_node(state: TreeState):
    """
    ã€åˆå§‹åŒ–èŠ‚ç‚¹ã€‘ï¼šåˆ›å»ºæ ‘æ ¹
    ä½¿ç”¨å½’ä¸€åŒ–åçš„æŸ¥è¯¢ä½œä¸º MCTS çš„èµ·ç‚¹ã€‚
    """
    q = state.get("normalized_query", state["original_query"])
    print(f"\nğŸŒ± [Start] åˆå§‹åŒ– LATS æ ‘æœç´¢ï¼ŒåŸºå‡†æŸ¥è¯¢: {q}")
    
    root = MCTSNode(action_thought=q)
    # åˆå§‹åŒ–çŠ¶æ€ï¼šå½“å‰èŠ‚ç‚¹æŒ‡å‘ Rootï¼Œè¿­ä»£è®¡æ•°å½’é›¶
    return {"root": root, "current_node": root, "iterations": 0}

def selection_node(state: TreeState):
    """
    ã€é€‰æ‹©èŠ‚ç‚¹ (Selection)ã€‘
    åŸºäº UCT ç®—æ³•ï¼Œä» Root å¼€å§‹ä¸€ç›´å¾€ä¸‹èµ°ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ª'æœ€æœ‰æ½œåŠ›'çš„å¶å­èŠ‚ç‚¹ã€‚
    """
    root = state["root"]
    node = root
    
    # è´ªå©ªé€‰æ‹©ï¼šåªè¦æœ‰å­©å­ï¼Œå°±é€‰ UCT åˆ†æ•°æœ€é«˜çš„é‚£ä¸ªå­©å­å¾€ä¸‹èµ°
    while node.children:
        node = max(node.children, key=lambda c: c.uct_score(node.visits))
    
    # è¿”å›é€‰ä¸­çš„èŠ‚ç‚¹ï¼Œå‡†å¤‡å¯¹å…¶è¿›è¡Œæ‰©å±•
    return {"current_node": node}

def expansion_node(state: TreeState):
    """
    ã€æ‰©å±•èŠ‚ç‚¹ (Expansion)ã€‘
    å¦‚æœå½“å‰èŠ‚ç‚¹è¿˜æ²¡è¢«è®¿é—®è¿‡ï¼Œæˆ–è€…å·²ç»è®¿é—®è¿‡ä½†éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå°±æ‰©å±•å®ƒã€‚
    LLM è´Ÿè´£ç”Ÿæˆ 2-3 ä¸ªå…·ä½“çš„å­æŸ¥è¯¢ã€‚
    """
    current_node = state["current_node"]
    query = current_node.action_thought
    
    # é˜²æ­¢æ— é™é€’å½’ï¼šé™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦ä¸º 3
    if (current_node.visits > 0 or current_node.depth == 0) and current_node.depth < 3:
        print(f"ğŸŒ² [Expand] æ­£åœ¨æ‰©å±•èŠ‚ç‚¹: '{query[:20]}...'")
        
        # --- Prompt ä¼˜åŒ–ï¼šæ³¨å…¥è¡Œä¸šçŸ¥è¯† ---
        # å¼ºåˆ¶è¦æ±‚ç”Ÿæˆå¸¦æœ‰ WPM (äº§èƒ½å•ä½)ã€åŒºåˆ†è¥æ”¶/äº§èƒ½çš„æŸ¥è¯¢
        prompt = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åŠå¯¼ä½“è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚é’ˆå¯¹é—®é¢˜: "{query}"
        è¯·ç”Ÿæˆ 2 åˆ° 3 ä¸ª æå…·é’ˆå¯¹æ€§çš„æœç´¢å­æŸ¥è¯¢ï¼Œæ—¨åœ¨æŒ–æ˜æ·±å±‚æ•°æ®ã€‚
        
        è¦æ±‚ï¼š
        1. **ç²¾ç¡®åŒ–åº¦é‡è¡¡**ï¼šå¦‚æœæ¶‰åŠäº§èƒ½ï¼Œå¿…é¡»åŒ…å« "WPM" (Wafers Per Month), "capacity utilization" (äº§èƒ½åˆ©ç”¨ç‡) ç­‰å…³é”®è¯ã€‚
        2. **åŒºåˆ†æ¦‚å¿µ**ï¼šæ˜ç¡®åŒºåˆ† "Revenue Share" (è¥æ”¶å æ¯”) å’Œ "Wafer Allocation" (æ™¶åœ†é…é¢/äº§èƒ½å æ¯”)ï¼Œé¿å…æ··æ·†ã€‚
        3. **å…·ä½“å·¥è‰ºèŠ‚ç‚¹**ï¼šå¯¹äºå…ˆè¿›åˆ¶ç¨‹ï¼Œå°è¯•åŠ å…¥å…·ä½“ä»£å·ï¼ˆå¦‚ TSMC N3, N3E, N3B, N3Pï¼‰ã€‚
        4. **æƒå¨æ¥æºå¯¼å‘**ï¼šå¯ä»¥åŠ ä¸Š "report", "TrendForce", "Digitimes" ç­‰å…³é”®è¯ä»¥å¼•å¯¼æœç´¢é«˜è´¨é‡ç ”æŠ¥ã€‚
        
        è¯·ä¸¥æ ¼è¿”å› Python åˆ—è¡¨æ ¼å¼å­—ç¬¦ä¸²ã€‚
        ç¤ºä¾‹: ["TSMC N3 capacity WPM 2024 forecast", "Apple vs Nvidia TSMC 3nm wafer allocation 2024"]
        """
        try:
            response = llm.invoke(prompt).content
            # è§£æ LLM è¿”å›çš„ List å­—ç¬¦ä¸²
            start = response.find('[')
            end = response.rfind(']') + 1
            sub_queries = ast.literal_eval(response[start:end])
        except Exception as e:
            print(f"âš ï¸ è§£ææ‰©å±•æŸ¥è¯¢å¤±è´¥: {e}")
            sub_queries = [f"{query} WPM details", f"{query} market share report"]

        if not sub_queries:
            sub_queries = [query]

        # å°†ç”Ÿæˆçš„å­æŸ¥è¯¢æŒ‚è½½ä¸ºå½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
        for q in sub_queries:
            child = MCTSNode(parent=current_node, action_thought=q)
            current_node.add_child(child)
        
        # æ‰©å±•å®Œåï¼Œç«‹åˆ»é€‰æ‹©ç¬¬ä¸€ä¸ªæ–°ç”Ÿæˆçš„å­èŠ‚ç‚¹è¿›å…¥ä¸‹ä¸€æ­¥(Simulation)
        if current_node.children:
            return {"current_node": current_node.children[0]}
    
    return {"current_node": current_node}

def simulation_node(state: TreeState):
    """
    ã€æ¨¡æ‹ŸèŠ‚ç‚¹ (Simulation)ã€‘
    æ‰§è¡ŒçœŸå®çš„æ£€ç´¢åŠ¨ä½œï¼Œå¹¶è¿›è¡Œ'äº‹å®é”šå®š'ã€‚
    è¿™é‡ŒåŒ…å«äº† RAG æ£€ç´¢ã€Web æœç´¢å’Œä¸­é—´ç­”æ¡ˆç”Ÿæˆã€‚
    """
    node = state["current_node"]
    
    # ç¼“å­˜æœºåˆ¶ï¼šå¦‚æœè¯¥èŠ‚ç‚¹å·²ç»ç”Ÿæˆè¿‡ç­”æ¡ˆï¼Œç›´æ¥è·³è¿‡
    if node.generated_answer:
        return {"current_node": node}
        
    query = node.action_thought
    print(f"ğŸ” [Simulate] æ‰§è¡ŒçœŸå®æ£€ç´¢: {query}")
    
    raw_docs = [] 
    
    # 1. æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ (RAG)
    try:
        rag_docs = ensemble_retriever.invoke(query)
        if rag_docs:
            for d in rag_docs[:2]:
                source = d.metadata.get("source", "Local Doc")
                raw_docs.append(f"ã€æœ¬åœ°-{source}ã€‘: {d.page_content}")
    except Exception as e:
        print(f"   -> RAG é”™è¯¯: {e}")

    # 2. ç½‘ç»œæœç´¢ (Google Serper)
    try:
        web_res = web_search_tool.results(query)
        if "organic" in web_res:
            for item in web_res["organic"][:2]:
                raw_docs.append(f"ã€ç½‘ç»œ-{item.get('title')}ã€‘: {item.get('snippet')}")
    except Exception as e:
        print(f"   -> Web é”™è¯¯: {e}")

    # 3. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡å¹¶ç¼–å·ï¼Œç”¨äºå¼•ç”¨
    if not raw_docs:
        formatted_context = "æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚"
        node.content = formatted_context
    else:
        formatted_parts = []
        for i, doc in enumerate(raw_docs):
            formatted_parts.append(f"[Ref: {i}] {doc}")
        formatted_context = "\n\n".join(formatted_parts)
        node.content = formatted_context

    # 4. ç”Ÿæˆå¸¦å¼•ç”¨çš„ä¸­é—´ç­”æ¡ˆ (å…³é”®ä¼˜åŒ–)
    # ä½¿ç”¨ temperature=0 å¼ºåˆ¶æ¨¡å‹ä¸¥æ ¼éµå®ˆå¼•ç”¨è§„åˆ™å’Œæ‹’ç­”æœºåˆ¶
    print(f"   -> æ­£åœ¨å°è¯•ç”Ÿæˆä¸­é—´ç­”æ¡ˆå¹¶è¿›è¡Œäº‹å®é”šå®š...")
    
    anchor_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠå¯¼ä½“äº§ä¸šç ”ç©¶å‘˜ã€‚è¯·åŸºäºä¸‹æ–¹çš„ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ï¼š"{query}"ã€‚

    ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘
    {formatted_context}

    ã€å›ç­”è§„åˆ™ã€‘
    1. **äº‹å®é”šå®š**ï¼šä½ ç”Ÿæˆçš„æ¯ä¸€å¥è¯ï¼Œå¦‚æœå¼•ç”¨äº†ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨æ¥æº IDï¼Œæ ¼å¼ä¸º [Ref: x]ã€‚
    2. **æ•°æ®æ•æ„Ÿæ€§**ï¼š
       - å¦‚æœæ‰¾åˆ°**å…·ä½“æ•°å­—**ï¼ˆå¦‚ "60k-70k wpm"ï¼‰ï¼Œè¯·åŠ¡å¿…ä¿ç•™å¹¶å¼•ç”¨ã€‚
       - å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯**è¥æ”¶å æ¯”**ï¼ˆRevenueï¼‰ï¼Œä¸¥ç¦å°†å…¶ç›´æ¥ç­‰åŒäº**äº§èƒ½å æ¯”**ï¼ˆCapacity/Waferï¼‰ï¼Œå¿…é¡»æ˜ç¡®æŒ‡å‡ºæ˜¯â€œè¥æ”¶â€è¿˜æ˜¯â€œäº§èƒ½â€ã€‚
    3. **é¢„æµ‹åŒ…å®¹æ€§**ï¼šå¦‚æœæ²¡æœ‰å®˜æ–¹æŠ«éœ²çš„ç²¾ç¡®æ•°æ®ï¼Œ**å…è®¸å¼•ç”¨çŸ¥ååˆ†ææœºæ„çš„ä¼°ç®—æ•°æ®**ï¼Œä½†å¿…é¡»åœ¨å›ç­”ä¸­æ˜ç¡®è¯´æ˜ã€‚
    4. **æ‹’ç­”æœºåˆ¶**ï¼šå¦‚æœä¸Šä¸‹æ–‡å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­” "æ— æ³•å›ç­”"ã€‚

    è¯·ç”Ÿæˆç²¾ç‚¼çš„å›ç­”ï¼š
    """
    
    try:
        simulation_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
        generated_answer = simulation_llm.invoke(anchor_prompt).content
    except Exception as e:
        generated_answer = "æ— æ³•å›ç­” (ç”Ÿæˆé”™è¯¯)"

    node.generated_answer = generated_answer
    print(f"   -> ä¸­é—´ç­”æ¡ˆ: {generated_answer[:50]}...")
    
    return {"current_node": node}

def evaluation_node(state: TreeState):
    """
    ã€è¯„ä¼°èŠ‚ç‚¹ (Evaluation) & åå‘ä¼ æ’­ (Backpropagation)ã€‘
    å¯¹ç”Ÿæˆå†…å®¹çš„è´¨é‡è¿›è¡Œæ‰“åˆ†ï¼Œå¹¶å°†åˆ†æ•°å›ä¼ ç»™æ‰€æœ‰çˆ¶èŠ‚ç‚¹ã€‚
    """
    node = state["current_node"]
    original_query = state["original_query"]
    answer = node.generated_answer
    
    print(f"âš–ï¸ [Evaluate] æ­£åœ¨è¯„ä¼°èŠ‚ç‚¹è´¨é‡...")
    
    score = 0.0
    
    # --- è¯„åˆ†ç­–ç•¥ ---
    
    # è§„åˆ™ 1: æ‹’ç­”å¤„ç† (ç»™ä½åˆ†ä½†ä¸æ˜¯0åˆ†ï¼Œé¼“åŠ±è¯šå®)
    if "æ— æ³•å›ç­”" in answer or "No information" in answer:
        print("   -> æ£€æµ‹åˆ°æ‹’ç­”ï¼Œç»™äºˆä½åˆ†å¥–åŠ± (0.1) ä»¥é¼“åŠ±è¯šå®ã€‚")
        score = 0.1
    
    # è§„åˆ™ 2: å¼•ç”¨æ£€æŸ¥ (äº‹å®é”šå®šéªŒè¯)
    elif "[Ref:" in answer:
        verify_prompt = f"""
        ä½œä¸ºåŠå¯¼ä½“æ•°æ®å®¡æ ¸å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹å›ç­”è¿›è¡Œä¸¥æ ¼æ‰“åˆ† (0.0 åˆ° 1.0)ã€‚
        
        ç”¨æˆ·åŸé—®é¢˜: {original_query}
        å½“å‰å­æŸ¥è¯¢: {node.action_thought}
        ç”Ÿæˆçš„å›ç­”: {answer}
        æ£€ç´¢åˆ°çš„åŸæ–‡: 
        {node.content[:2000]}
        
        è¯„åˆ†æ ‡å‡†ï¼š
        1. **ç›¸å…³æ€§** (0.4åˆ†)ï¼šå›ç­”æ˜¯å¦ç›´æ¥è§£å†³äº†å­æŸ¥è¯¢çš„æ ¸å¿ƒé—®é¢˜ï¼Ÿ
        2. **çœŸå®æ€§** (0.4åˆ†)ï¼š[Ref: x] çš„å¼•ç”¨æ˜¯å¦çœŸå®å­˜åœ¨äºåŸæ–‡ä¸­ï¼Œä¸”æ²¡æœ‰æ­ªæ›²åŸæ„ï¼Ÿ
        3. **ç²¾å‡†åº¦** (0.2åˆ†)ï¼š
           - å¦‚æœå›ç­”åŒ…å«äº†**å…·ä½“æ•°å€¼**ï¼ˆå¦‚ "80k wpm"ï¼‰è€Œä¸æ˜¯æ¨¡ç³Šæè¿°ï¼ŒåŠ åˆ†ã€‚
           - å¦‚æœå›ç­”**æ··æ·†äº†è¥æ”¶ï¼ˆRevenueï¼‰å’Œäº§èƒ½ï¼ˆWafer/Capacityï¼‰**ï¼Œç›´æ¥æ‰£é™¤ 0.5 åˆ†ï¼ˆä¸¥é‡é”™è¯¯ï¼‰ã€‚
        
        è¯·ç»¼åˆè€ƒè™‘åï¼Œåªè¿”å›ä¸€ä¸ªæ•°å­—ã€‚
        """
        try:
            response = llm.invoke(verify_prompt).content.strip()
            match = re.search(r"0\.\d+|1\.0|0|1", response)
            if match:
                score = float(match.group())
        except:
            score = 0.5 # å‡ºé”™å›é€€
        print(f"   -> å¼•ç”¨æ ¡éªŒå¾—åˆ†: {score}")
        
    # è§„åˆ™ 3: æ— å¼•ç”¨æƒ©ç½š (æ½œåœ¨å¹»è§‰)
    else:
        print("   -> âš ï¸ è­¦å‘Š: å›ç­”æœªåŒ…å«å¼•ç”¨ï¼Œåˆ¤å®šä¸ºæ½œåœ¨å¹»è§‰ï¼Œç»™äºˆ 0 åˆ†ã€‚")
        score = 0.0

    # --- åå‘ä¼ æ’­ (Backpropagation) ---
    # æ ¸å¿ƒé€»è¾‘ï¼šå°†åˆ†æ•°åŠ åˆ°å½“å‰èŠ‚ç‚¹åŠå…¶æ‰€æœ‰ç¥–å…ˆèŠ‚ç‚¹çš„ value_sum ä¸­
    # è¿™ä¼šå½±å“åç»­ Selection é˜¶æ®µ UCT åˆ†æ•°çš„è®¡ç®—
    temp_node = node
    while temp_node:
        temp_node.visits += 1
        temp_node.value_sum += score
        temp_node = temp_node.parent
        
    return {"iterations": state["iterations"] + 1}

def generation_node(state: TreeState):
    """
    ã€ç”ŸæˆèŠ‚ç‚¹ (Generation)ã€‘
    åœ¨æ‰€æœ‰è¿­ä»£ç»“æŸåï¼Œæ”¶é›†é«˜åˆ†è·¯å¾„çš„ä¿¡æ¯ï¼Œæ±‡æ€»ç”Ÿæˆæœ€ç»ˆç ”æŠ¥ã€‚
    """
    print("\nâœï¸ [Generate] æœç´¢ç»“æŸï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    root = state["root"]
    original_query = state["original_query"]
    
    all_contexts = []
    
    # é€’å½’æ”¶é›†é«˜ä»·å€¼ä¿¡æ¯
    def collect_contexts(node: MCTSNode):
        # è¿‡æ»¤æ¡ä»¶ï¼šå¿…é¡»è®¿é—®è¿‡ã€å¾—åˆ†è¾ƒé«˜ (>0.4)ã€ä¸”ä¸æ˜¯æ‹’ç­”å†…å®¹
        if node.visits > 0 and node.value > 0.4 and node.generated_answer:
            if "æ— æ³•å›ç­”" not in node.generated_answer:
                all_contexts.append(f"ã€æ¥æº: {node.action_thought}ã€‘\n{node.generated_answer}")
        
        for child in node.children:
            collect_contexts(child)
            
    collect_contexts(root)
    
    # å»é‡å¹¶æ‹¼æ¥ï¼Œé™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    unique_contexts = "\n\n".join(list(set(all_contexts))[:10])
    
    if not unique_contexts:
        unique_contexts = "æœªæ£€ç´¢åˆ°è¶³å¤Ÿçš„å¯ä¿¡ä¿¡æ¯ã€‚"

    # æœ€ç»ˆç”Ÿæˆçš„ Promptï¼šè¦æ±‚ç»“æ„åŒ–ã€ç ”æŠ¥é£æ ¼
    final_prompt = f"""
    ä½ æ˜¯ç”±å­—èŠ‚è·³åŠ¨ RAG æŠ€æœ¯æ”¯æŒçš„åŠå¯¼ä½“é¦–å¸­æˆ˜ç•¥é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹**ç»è¿‡äº‹å®æ ¸æŸ¥**çš„ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½å…³äº "{original_query}" çš„ç®€æŠ¥ã€‚
    
    === ç»è¿‡æ ¸æŸ¥çš„æƒ…æŠ¥ç¢ç‰‡ ===
    {unique_contexts}
    =========================
    
    æ’°å†™è¦æ±‚ï¼š
    1. **ç»“æ„åŒ–è¾“å‡º**ï¼šè¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«ã€æ ¸å¿ƒç»“è®ºã€‘ã€ã€æ•°æ®è¯¦è§£ã€‘ã€ã€é£é™©/ä¸ç¡®å®šæ€§æç¤ºã€‘ä¸‰ä¸ªéƒ¨åˆ†ã€‚
    2. **æ•°æ®ç²¾å‡†**ï¼šä¼˜å…ˆå±•ç¤ºå…·ä½“æ•°å­—ï¼ˆå¦‚äº§èƒ½ WPMã€è‰¯ç‡ %ï¼‰ã€‚å¦‚æœå¼•ç”¨çš„æ˜¯åˆ†ææœºæ„ï¼ˆå¦‚ TrendForceï¼‰çš„ä¼°ç®—å€¼ï¼Œè¯·æ˜ç¡®æ ‡æ³¨â€œä¼°ç®—â€ã€‚
    3. **æ¦‚å¿µå˜æ¸…**ï¼šåœ¨æè¿°å æ¯”æ—¶ï¼Œæ˜ç¡®åŒºåˆ†æ˜¯â€œè¥æ”¶è´¡çŒ®å æ¯”â€è¿˜æ˜¯â€œæ™¶åœ†äº§èƒ½å æ¯”â€ï¼Œè‹¥æ•°æ®ç¼ºå¤±è¯·è¯´æ˜ã€‚
    4. **æ¥æºæ ‡æ³¨**ï¼šåœ¨å…³é”®æ•°æ®åä¿ç•™ [Ref] æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œæˆ–è¯´æ˜æ¥æºäºå“ªä¸ªå­æŸ¥è¯¢ã€‚
    5. **å»ä¼ªå­˜çœŸ**ï¼šå¦‚æœç¢ç‰‡ä¿¡æ¯ä¸­å­˜åœ¨å†²çªï¼Œè¯·å¯¹æ¯”å±•ç¤ºï¼Œä¸è¦å¼ºè¡Œåˆå¹¶ã€‚
    
    æœ€ç»ˆç®€æŠ¥ï¼š
    """
    
    final_answer = llm.invoke(final_prompt).content
    return {"best_answer": final_answer}

# ==========================================
# 3. è·¯ç”±é€»è¾‘ (Control Flow)
# ==========================================

def should_continue(state: TreeState):
    """
    æ¡ä»¶è¾¹é€»è¾‘ï¼šåˆ¤æ–­æ˜¯ç»§ç»­è¿­ä»£è¿˜æ˜¯ç»“æŸã€‚
    ç”± state['max_iterations'] å†³å®šå¾ªç¯æ¬¡æ•°ã€‚
    """
    if state["iterations"] < state["max_iterations"]:
        return "selection"  # æ²¡è¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œå›åˆ° Selection èŠ‚ç‚¹ç»§ç»­æœç´¢
    return "generate"       # è¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œè¿›å…¥ Generate èŠ‚ç‚¹è¾“å‡ºç»“æœ

# ==========================================
# 4. æ„å»º LangGraph å·¥ä½œæµ
# ==========================================

workflow = StateGraph(TreeState)

# --- æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹ ---
workflow.add_node("normalize", normalize_node) # å…¥å£ï¼šé¢„å¤„ç†
workflow.add_node("initial", initial_node)     # åˆå§‹åŒ–
workflow.add_node("selection", selection_node) # å¾ªç¯èµ·ç‚¹
workflow.add_node("expansion", expansion_node)
workflow.add_node("simulation", simulation_node)
workflow.add_node("evaluation", evaluation_node) # å¾ªç¯ç»ˆç‚¹
workflow.add_node("generate", generation_node)   # å‡ºå£ï¼šç”Ÿæˆ

# --- è®¾ç½®è¾¹çš„è¿æ¥å…³ç³» ---
# 1. è®¾ç½®å…¥å£
workflow.set_entry_point("normalize")

# 2. çº¿æ€§æµç¨‹
workflow.add_edge("normalize", "initial")
workflow.add_edge("initial", "selection")
workflow.add_edge("selection", "expansion")
workflow.add_edge("expansion", "simulation")
workflow.add_edge("simulation", "evaluation")

# 3. å¾ªç¯æ§åˆ¶ (Conditional Edge)
# åœ¨ evaluation ç»“æŸåï¼Œæ ¹æ® should_continue çš„è¿”å›å€¼å†³å®šå»å‘
workflow.add_conditional_edges(
    "evaluation",
    should_continue,
    {
        "selection": "selection", # ç»§ç»­å¾ªç¯
        "generate": "generate"    # ç»“æŸå¾ªç¯
    }
)

workflow.add_edge("generate", END)

# ç¼–è¯‘å›¾
app = workflow.compile()

# ==========================================
# 5. ä¸»ç¨‹åºæ‰§è¡Œå…¥å£
# ==========================================

if __name__ == "__main__":
    # æµ‹è¯• Queryï¼šåŒ…å«ä¸€äº›éæ ‡å‡†å†™æ³• (tsmc, 3nm)
    query = "2024å¹´tsmcçš„æœ€æ–°3nmäº§èƒ½æƒ…å†µå¦‚ä½•ï¼Ÿè‹¹æœå’Œnvçš„è®¢å•å æ¯”å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
    
    print(f"ğŸš€ [Agent Start] å¯åŠ¨ MCTS Agent (èŠ¯çŸ¥ - ä¸“ä¸šåŠå¯¼ä½“ç‰ˆ)")
    print(f"â“ åŸå§‹é—®é¢˜: {query}\n" + "="*50)
    
    inputs = {
        "original_query": query, 
        # è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå»ºè®® 5-10 æ¬¡ä»¥ä¿è¯æœç´¢è¦†ç›–åº¦
        "max_iterations": 6 
    }
    
    # è®¾ç½®é€’å½’é™åˆ¶ï¼Œé˜²æ­¢ GraphRecursionError
    config = {"recursion_limit": 50} 
    
    try:
        # å¯åŠ¨å›¾æ‰§è¡Œ
        final_state = app.invoke(inputs, config=config)
        
        print("\n" + "="*50)
        print("âœ… [Done] æœ€ç»ˆç­”æ¡ˆï¼š\n")
        print(final_state["best_answer"])
        print("\n" + "="*50)
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()