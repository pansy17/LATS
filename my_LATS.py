import os
import math
import ast
import operator
import re
from typing import List, TypedDict, Optional, Dict, Any, Union

from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper
from langgraph.graph import StateGraph, END

from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

openai_api_key = os.environ.get("OPENAI_API_KEY")
openai_api_base = os.environ.get("OPENAI_API_BASE")
serper_api_key = os.environ.get("SERPER_API_KEY")

from documents.retriever_tools import ensemble_retriever
print("æˆåŠŸåŠ è½½æœ¬åœ°RAGæ£€ç´¢å™¨")

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    openai_api_key=openai_api_key,
    openai_api_base=openai_api_base,
)
web_search_tool = GoogleSerperAPIWrapper(serper_api_key=serper_api_key)
print("æˆåŠŸåŠ è½½Google Serper")

class MCTSNode:
    def __init__(self,parent = None,action_thought:str = "Root"):
        self.parent: Optional[MCTSNode] = parent
        self.children: List[MCTSNode] = []

        self.action_thought = action_thought
        self.content: str = ""
        self.generated_answer: str = ""

        self.visits: int = 0
        self.total_reward: float = 0.0
        self.depth:int = 0 if parent is None else parent.depth + 1

    @property
    def value(self)->float:
        if self.visits == 0:
            return 0.0
        return self.total_reward / self.visits
    
    def uct_score(self, parent_visits:int,c_put:float = 1.41)->float:
        """è®¡ç®— UCT åˆ†æ•°"""
        if self.visits == 0:
            return float('inf')
        q_value = self.value
        u_value = c_put * math.sqrt(math.log(parent_visits) / self.visits)
        return q_value + u_value

    def add_child(self,child_node):
        self.children.append(child_node)
    
    def __repr__(self):
        return f"<Node depth={self.depth} action={self.action_thought} visits={self.visits} value={self.value:.2f}>"

class TreeState(TypedDict):
    original_query: str
    normalized_query: str
    root:MCTSNode
    current_node:MCTSNode
    iterations:int
    max_iterations:int
    best_answer:str
    
def normalize_node(state: TreeState):
    """
    ã€é¢„å¤„ç†èŠ‚ç‚¹ã€‘ï¼šæŸ¥è¯¢å½’ä¸€åŒ–
    ç›®çš„ï¼šè§£å†³åŠå¯¼ä½“é¢†åŸŸæœ¯è¯­ä¹¦å†™ä¸è§„èŒƒå¯¼è‡´çš„æ£€ç´¢æ¼å¬å›é—®é¢˜ã€‚
    ç­–ç•¥ï¼šæ­£åˆ™åŒ¹é… (è§„åˆ™) + LLM é‡å†™ (è¯­ä¹‰)ã€‚
    """ 
    query = state['original_query']
    print(f"æ­£åœ¨å½’ä¸€åŒ–æŸ¥è¯¢ï¼š{query}")

    term_mapping = {
        "3nm process": "3nm å·¥è‰º",
        "GAA": "GAA å·¥è‰º",
        "GaAs": "GaAs ææ–™",
        "GaN": "GaN ææ–™",
        "SiC": "SiC ææ–™",
        "Si": "Si ææ–™",
        "W": "W ææ–™",
        "SiGe": "SiGe ææ–™",
        "SiN": "SiN ææ–™",
        "InN": "InN ææ–™",
        "InP": "InP ææ–™",
        "AlN": "AlN ææ–™",
        "AlP": "AlP ææ–™",
        "AlAs": "AlAs ææ–™",
        "AlGaAs": "AlGaAs ææ–™",
        "AlGaN": "AlGaN ææ–™",
        "AlSi": "AlSi ææ–™",
        "AlW": "AlW ææ–™",
        "AlSiGe": "AlSiGe ææ–™",
        "AlSiN": "AlSiN ææ–™",
        "InGaAs": "InGaAs ææ–™",
        "InGaN": "InGaN ææ–™",
        "InSi": "InSi ææ–™",
        "InW": "InW ææ–™",
        "InSiGe": "InSiGe ææ–™",
        "InSiN": "InSiN ææ–™",
    }
    normalized_query = query
    for pattern, replacement in term_mapping.items():
        normalized_query = re.sub(pattern, replacement, normalized_query)
    # 2. LLM è¯­ä¹‰é‡å†™ - å¤„ç†å¤æ‚è¯­ä¹‰å’Œå®ä½“è¡¥å…¨
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŠå¯¼ä½“é¢†åŸŸçš„æœ¯è¯­ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
    
    åŸå§‹æŸ¥è¯¢: "{normalized_query}"
    
    ä»»åŠ¡ï¼š
    1. **çº æ­£æ‹¼å†™**ï¼šä¿®å¤æ˜æ˜¾çš„æ‹¼å†™é”™è¯¯ã€‚
    2. **æœ¯è¯­è§„èŒƒ**ï¼šå°†éæ ‡å‡†æè¿°è½¬æ¢ä¸ºè¡Œä¸šé€šç”¨æœ¯è¯­ï¼ˆå¦‚ "3çº³ç±³å·¥è‰º" -> "3nm Process Node"ï¼‰ã€‚
    3. **å®ä½“è¡¥å…¨**ï¼šå¦‚æœå®ä½“åç§°æ¨¡ç³Šï¼Œå°è¯•è¡¥å…¨ï¼ˆå¦‚ "A17" -> "Apple A17 Pro"ï¼‰ï¼Œä½†ä¸è¦æ”¹å˜åŸæ„ã€‚
    4. **å»å£è¯­åŒ–**ï¼šå»é™¤æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼Œä¿ç•™æ ¸å¿ƒæœç´¢æ„å›¾ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ ‡å‡†åŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å¼•å·ã€‚
    """ 

    
    try:
        response = llm.invoke(prompt).content.strip()
        final_query = response.strip('"').strip("'")
    except Exception as e:
        print(f"âš ï¸ å½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ç»“æœ: {e}")
        final_query = normalized_query

    print(f"   -> æ ‡å‡†åŒ–ç»“æœ: {final_query}")
    return {"normalized_query": final_query}

def initial_node(state: TreeState):
    q = state.get('normalized_query', state["original_query"])
    print(f"æ­£åœ¨åˆå§‹åŒ–æ ¹èŠ‚ç‚¹ï¼ŒæŸ¥è¯¢ä¸ºï¼š{q}")
    root = MCTSNode(action_thought=q)
    # âœ… ä¿®æ­£ï¼šå¿…é¡»è¿”å›æ›´æ–°åçš„ State
    return {"root": root, "current_node": root, "iterations": 0}

def selection_node(state:TreeState):
    """
    ã€é€‰æ‹©èŠ‚ç‚¹ã€‘ï¼šæ ¹æ® UCT åˆ†æ•°é€‰æ‹©æœ€ä¼˜å­èŠ‚ç‚¹
    ç­–ç•¥ï¼šé€’å½’éå†å­èŠ‚ç‚¹ï¼Œé€‰æ‹© UCT åˆ†æ•°æœ€é«˜çš„èŠ‚ç‚¹ã€‚
    """
    root = state["root"]
    node = root

    while node.children:
        node = max(node.children,key=lambda c:c.uct_score(node.visits))
    
    return {"current_node":node}

def expansion_node(state:TreeState):
    """
    ã€æ‰©å±•èŠ‚ç‚¹ã€‘ï¼šä¸ºå½“å‰èŠ‚ç‚¹æ·»åŠ å­èŠ‚ç‚¹
    ç­–ç•¥ï¼šæ ¹æ®å½“å‰èŠ‚ç‚¹çš„æŸ¥è¯¢ï¼Œä½¿ç”¨ LLM ç”Ÿæˆå­æŸ¥è¯¢ã€‚
    """
    current_node = state["current_node"]
    query = current_node.action_thought

    if (current_node.visits > 0 or current_node.depth == 0) and current_node.depth < 3:
        print(f"ğŸŒ² [Expand] æ­£åœ¨æ‰©å±•èŠ‚ç‚¹: '{query[:20]}...'")
        prompt = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åŠå¯¼ä½“è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚é’ˆå¯¹é—®é¢˜: "{query}"
        è¯·ç”Ÿæˆ 2 åˆ° 3 ä¸ª æå…·é’ˆå¯¹æ€§çš„æœç´¢å­æŸ¥è¯¢ï¼Œæ—¨åœ¨æŒ–æ˜æ·±å±‚æ•°æ®ã€‚
        
        è¦æ±‚ï¼š
        1. **ç²¾ç¡®åŒ–åº¦é‡è¡¡**ï¼šå¦‚æœæ¶‰åŠäº§èƒ½ï¼Œå¿…é¡»åŒ…å« "WPM" (Wafers Per Month), "capacity utilization" (äº§èƒ½åˆ©ç”¨ç‡) ç­‰å…³é”®è¯ã€‚
        2. **åŒºåˆ†æ¦‚å¿µ**ï¼šæ˜ç¡®åŒºåˆ† "Revenue Share" (è¥æ”¶å æ¯”) å’Œ "Wafer Allocation" (æ™¶åœ†é…é¢/äº§èƒ½å æ¯”)ï¼Œé¿å…æ··æ·†ã€‚
        3. **å…·ä½“å·¥è‰ºèŠ‚ç‚¹**ï¼šå¯¹äºå…ˆè¿›åˆ¶ç¨‹ï¼Œå°è¯•åŠ å…¥å…·ä½“ä»£å·ï¼ˆå¦‚ TSMC N3, N3E, N3B, N3Pï¼‰ã€‚
        4. **æƒå¨æ¥æºå¯¼å‘**ï¼šå¯ä»¥åŠ ä¸Š "report", "TrendForce", "Digitimes" ç­‰å…³é”®è¯ä»¥å¼•å¯¼æœç´¢é«˜è´¨é‡ç ”æŠ¥ã€‚
        
        è¯·ä¸¥æ ¼è¿”å› Python åˆ—è¡¨æ ¼å¼å­—ç¬¦ä¸²ã€‚
        ç¤ºä¾‹: ["TSMC N3 capacity WPM 2024 forecast", "Apple vs Nvidia TSMC 3nm wafer allocation 2024"]
        """
        try:
            response = llm.invoke(prompt).content
            # è§£æ LLM è¿”å›çš„ List å­—ç¬¦ä¸²
            start = response.find('[')
            end = response.rfind(']') + 1
            sub_queries = ast.literal_eval(response[start:end])
        except Exception as e:
            print(f"âš ï¸ è§£ææ‰©å±•æŸ¥è¯¢å¤±è´¥: {e}")
            sub_queries = [f"{query} WPM details", f"{query} market share report"]

        if not sub_queries:
            sub_queries = [query]

        # å°†ç”Ÿæˆçš„å­æŸ¥è¯¢æŒ‚è½½ä¸ºå½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
        for q in sub_queries:
            child = MCTSNode(parent=current_node, action_thought=q)
            current_node.add_child(child)
        
        # æ‰©å±•å®Œåï¼Œç«‹åˆ»é€‰æ‹©ç¬¬ä¸€ä¸ªæ–°ç”Ÿæˆçš„å­èŠ‚ç‚¹è¿›å…¥ä¸‹ä¸€æ­¥(Simulation)
        if current_node.children:
            return {"current_node": current_node.children[0]}
    
    return {"current_node": current_node}

def simulation_node(state:TreeState):
    """
    ã€æ¨¡æ‹ŸèŠ‚ç‚¹ (Simulation)ã€‘
    æ‰§è¡ŒçœŸå®çš„æ£€ç´¢åŠ¨ä½œï¼Œå¹¶è¿›è¡Œ'äº‹å®é”šå®š'ã€‚
    è¿™é‡ŒåŒ…å«äº† RAG æ£€ç´¢ã€Web æœç´¢å’Œä¸­é—´ç­”æ¡ˆç”Ÿæˆã€‚
    """
    node = state["current_node"]

    if node.generated_answer:
        return {"current_node": node}
    query = node.action_thought
    print(f"ğŸŒ² [Simulate] æ­£åœ¨æ£€ç´¢{query}'")
    raw_docs = []
    try:
        rag_docs = ensemble_retriever.invoke(query)
        if rag_docs:
            for d in rag_docs[:2]:
                source = d.metadata.get("source", "Local Doc")
                raw_docs.append(f"ã€æœ¬åœ°-{source}ã€‘: {d.page_content}")
    except Exception as e:
        print(f"   -> RAG é”™è¯¯: {e}")

    try:
        web_res = web_search_tool.results(query)
        if "organic" in web_res:
            for item in web_res["organic"][:2]:
                raw_docs.append(f"ã€ç½‘ç»œ-{item.get('title')}ã€‘: {item.get('snippet')}")
    except Exception as e:
        print(f"   -> Web é”™è¯¯: {e}")

    if not raw_docs:
        formatted_context = "æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚"
        node.content = formatted_context
    else:
        formatted_parts = []
        for i, doc in enumerate(raw_docs):
            formatted_parts.append(f"[Ref: {i}] {doc}")
        formatted_context = "\n\n".join(formatted_parts)
        node.content = formatted_context
    # 4. ç”Ÿæˆå¸¦å¼•ç”¨çš„ä¸­é—´ç­”æ¡ˆ (å…³é”®ä¼˜åŒ–)
    # ä½¿ç”¨ temperature=0 å¼ºåˆ¶æ¨¡å‹ä¸¥æ ¼éµå®ˆå¼•ç”¨è§„åˆ™å’Œæ‹’ç­”æœºåˆ¶
    print(f"   -> æ­£åœ¨å°è¯•ç”Ÿæˆä¸­é—´ç­”æ¡ˆå¹¶è¿›è¡Œäº‹å®é”šå®š...")
    
    anchor_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠå¯¼ä½“äº§ä¸šç ”ç©¶å‘˜ã€‚è¯·åŸºäºä¸‹æ–¹çš„ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ï¼š"{query}"ã€‚

    ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘
    {formatted_context}

    ã€å›ç­”è§„åˆ™ã€‘
    1. **äº‹å®é”šå®š**ï¼šä½ ç”Ÿæˆçš„æ¯ä¸€å¥è¯ï¼Œå¦‚æœå¼•ç”¨äº†ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨æ¥æº IDï¼Œæ ¼å¼ä¸º [Ref: x]ã€‚
    2. **æ•°æ®æ•æ„Ÿæ€§**ï¼š
       - å¦‚æœæ‰¾åˆ°**å…·ä½“æ•°å­—**ï¼ˆå¦‚ "60k-70k wpm"ï¼‰ï¼Œè¯·åŠ¡å¿…ä¿ç•™å¹¶å¼•ç”¨ã€‚
       - å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯**è¥æ”¶å æ¯”**ï¼ˆRevenueï¼‰ï¼Œä¸¥ç¦å°†å…¶ç›´æ¥ç­‰åŒäº**äº§èƒ½å æ¯”**ï¼ˆCapacity/Waferï¼‰ï¼Œå¿…é¡»æ˜ç¡®æŒ‡å‡ºæ˜¯â€œè¥æ”¶â€è¿˜æ˜¯â€œäº§èƒ½â€ã€‚
    3. **é¢„æµ‹åŒ…å®¹æ€§**ï¼šå¦‚æœæ²¡æœ‰å®˜æ–¹æŠ«éœ²çš„ç²¾ç¡®æ•°æ®ï¼Œ**å…è®¸å¼•ç”¨çŸ¥ååˆ†ææœºæ„çš„ä¼°ç®—æ•°æ®**ï¼Œä½†å¿…é¡»åœ¨å›ç­”ä¸­æ˜ç¡®è¯´æ˜ã€‚
    4. **æ‹’ç­”æœºåˆ¶**ï¼šå¦‚æœä¸Šä¸‹æ–‡å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­” "æ— æ³•å›ç­”"ã€‚

    è¯·ç”Ÿæˆç²¾ç‚¼çš„å›ç­”ï¼š
    """
    
    try:
        simulation_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
        generated_answer = simulation_llm.invoke(anchor_prompt).content
    except Exception as e:
        generated_answer = "æ— æ³•å›ç­” (ç”Ÿæˆé”™è¯¯)"

    node.generated_answer = generated_answer
    print(f"   -> ä¸­é—´ç­”æ¡ˆ: {generated_answer[:50]}...")
    
    return {"current_node": node}

def evaluation_node(state:TreeState):
    node = state["current_node"]
    original_query = state["original_query"]
    answer = node.generated_answer

    print(f"[Evaluate] æ­£åœ¨è¯„ä¼°èŠ‚ç‚¹è´¨é‡...")

    if "æ— æ³•å›ç­”" in answer or "No information" in answer:
        print("   -> æ£€æµ‹åˆ°æ‹’ç­”ï¼Œç»™äºˆä½åˆ†å¥–åŠ± (0.1) ä»¥é¼“åŠ±è¯šå®ã€‚")
        score = 0.1

    elif "[Ref:" in answer:
        verify_prompt = f"""
        ä½œä¸ºåŠå¯¼ä½“æ•°æ®å®¡æ ¸å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹å›ç­”è¿›è¡Œä¸¥æ ¼æ‰“åˆ† (0.0 åˆ° 1.0)ã€‚
        
        ç”¨æˆ·åŸé—®é¢˜: {original_query}
        å½“å‰å­æŸ¥è¯¢: {node.action_thought}
        ç”Ÿæˆçš„å›ç­”: {answer}
        æ£€ç´¢åˆ°çš„åŸæ–‡: 
        {node.content[:2000]}
        
        è¯„åˆ†æ ‡å‡†ï¼š
        1. **ç›¸å…³æ€§** (0.4åˆ†)ï¼šå›ç­”æ˜¯å¦ç›´æ¥è§£å†³äº†å­æŸ¥è¯¢çš„æ ¸å¿ƒé—®é¢˜ï¼Ÿ
        2. **çœŸå®æ€§** (0.4åˆ†)ï¼š[Ref: x] çš„å¼•ç”¨æ˜¯å¦çœŸå®å­˜åœ¨äºåŸæ–‡ä¸­ï¼Œä¸”æ²¡æœ‰æ­ªæ›²åŸæ„ï¼Ÿ
        3. **ç²¾å‡†åº¦** (0.2åˆ†)ï¼š
           - å¦‚æœå›ç­”åŒ…å«äº†**å…·ä½“æ•°å€¼**ï¼ˆå¦‚ "80k wpm"ï¼‰è€Œä¸æ˜¯æ¨¡ç³Šæè¿°ï¼ŒåŠ åˆ†ã€‚
           - å¦‚æœå›ç­”**æ··æ·†äº†è¥æ”¶ï¼ˆRevenueï¼‰å’Œäº§èƒ½ï¼ˆWafer/Capacityï¼‰**ï¼Œç›´æ¥æ‰£é™¤ 0.5 åˆ†ï¼ˆä¸¥é‡é”™è¯¯ï¼‰ã€‚
        
        è¯·ç»¼åˆè€ƒè™‘åï¼Œåªè¿”å›ä¸€ä¸ªæ•°å­—ã€‚
        """
        try:
            response = llm.invoke(verify_prompt).content.strip()
            match = re.search(r"0\.\d+|1\.0|0|1", response)
            if match:
                score = float(match.group())
        except:
            score = 0.5 # å‡ºé”™å›é€€
        print(f"   -> å¼•ç”¨æ ¡éªŒå¾—åˆ†: {score}")
        
    # è§„åˆ™ 3: æ— å¼•ç”¨æƒ©ç½š (æ½œåœ¨å¹»è§‰)
    else:
        print("   -> âš ï¸ è­¦å‘Š: å›ç­”æœªåŒ…å«å¼•ç”¨ï¼Œåˆ¤å®šä¸ºæ½œåœ¨å¹»è§‰ï¼Œç»™äºˆ 0 åˆ†ã€‚")
        score = 0.0

    temp_node = node


    while temp_node:  # âœ… ä¿®æ­£ï¼šåªè¦èŠ‚ç‚¹å­˜åœ¨å°±å¾ªç¯ï¼ˆåŒ…æ‹¬ Rootï¼‰
        temp_node.visits += 1        # âœ… ä¿®æ­£ï¼šå…ˆæ›´æ–°å½“å‰èŠ‚ç‚¹
        temp_node.total_reward += score
        temp_node = temp_node.parent # âœ… ä¿®æ­£ï¼šç„¶åå†å¾€ä¸Šèµ°
    
    return {"iterations": state["iterations"] + 1}

def generation_node(state:TreeState):
    """
    ã€ç”ŸæˆèŠ‚ç‚¹ (Generation)ã€‘
    åœ¨æ‰€æœ‰è¿­ä»£ç»“æŸåï¼Œæ”¶é›†é«˜åˆ†è·¯å¾„çš„ä¿¡æ¯ï¼Œæ±‡æ€»ç”Ÿæˆæœ€ç»ˆç ”æŠ¥ã€‚
    """
    print   (f"ğŸŒ² [Generate] æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç ”æŠ¥...")

    root = state["root"]
    original_query = state["original_query"]

    all_contexts = []
    def collect_contexts(node:MCTSNode):
        if node.visits>0 and node.value>0.4 and node.generated_answer:
            if "æ— æ³•å›ç­”" not in node.generated_answer:
                all_contexts.append(f"ã€æ¥æº: {node.action_thought}ã€‘\n{node.generated_answer}")
        
        for child in node.children:
            collect_contexts(child)
    
    collect_contexts(root)

    unique_contexts = "\n\n".join(list(set(all_contexts))[:10])
    
    if not unique_contexts:
        unique_contexts = "æœªæ£€ç´¢åˆ°è¶³å¤Ÿçš„å¯ä¿¡ä¿¡æ¯ã€‚"

    # æœ€ç»ˆç”Ÿæˆçš„ Promptï¼šè¦æ±‚ç»“æ„åŒ–ã€ç ”æŠ¥é£æ ¼
    final_prompt = f"""
    ä½ æ˜¯ç”±å­—èŠ‚è·³åŠ¨ RAG æŠ€æœ¯æ”¯æŒçš„åŠå¯¼ä½“é¦–å¸­æˆ˜ç•¥é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹**ç»è¿‡äº‹å®æ ¸æŸ¥**çš„ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½å…³äº "{original_query}" çš„ç®€æŠ¥ã€‚
    
    === ç»è¿‡æ ¸æŸ¥çš„æƒ…æŠ¥ç¢ç‰‡ ===
    {unique_contexts}
    =========================
    
    æ’°å†™è¦æ±‚ï¼š
    1. **ç»“æ„åŒ–è¾“å‡º**ï¼šè¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«ã€æ ¸å¿ƒç»“è®ºã€‘ã€ã€æ•°æ®è¯¦è§£ã€‘ã€ã€é£é™©/ä¸ç¡®å®šæ€§æç¤ºã€‘ä¸‰ä¸ªéƒ¨åˆ†ã€‚
    2. **æ•°æ®ç²¾å‡†**ï¼šä¼˜å…ˆå±•ç¤ºå…·ä½“æ•°å­—ï¼ˆå¦‚äº§èƒ½ WPMã€è‰¯ç‡ %ï¼‰ã€‚å¦‚æœå¼•ç”¨çš„æ˜¯åˆ†ææœºæ„ï¼ˆå¦‚ TrendForceï¼‰çš„ä¼°ç®—å€¼ï¼Œè¯·æ˜ç¡®æ ‡æ³¨â€œä¼°ç®—â€ã€‚
    3. **æ¦‚å¿µå˜æ¸…**ï¼šåœ¨æè¿°å æ¯”æ—¶ï¼Œæ˜ç¡®åŒºåˆ†æ˜¯â€œè¥æ”¶è´¡çŒ®å æ¯”â€è¿˜æ˜¯â€œæ™¶åœ†äº§èƒ½å æ¯”â€ï¼Œè‹¥æ•°æ®ç¼ºå¤±è¯·è¯´æ˜ã€‚
    4. **æ¥æºæ ‡æ³¨**ï¼šåœ¨å…³é”®æ•°æ®åä¿ç•™ [Ref] æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œæˆ–è¯´æ˜æ¥æºäºå“ªä¸ªå­æŸ¥è¯¢ã€‚
    5. **å»ä¼ªå­˜çœŸ**ï¼šå¦‚æœç¢ç‰‡ä¿¡æ¯ä¸­å­˜åœ¨å†²çªï¼Œè¯·å¯¹æ¯”å±•ç¤ºï¼Œä¸è¦å¼ºè¡Œåˆå¹¶ã€‚
    
    æœ€ç»ˆç®€æŠ¥ï¼š
    """
    
    final_answer = llm.invoke(final_prompt).content
    return {"best_answer": final_answer}
def should_continue(state: TreeState):
    if state["iterations"] < state["max_iterations"]:
        return "selection"
    return "generate"

workflow = StateGraph(TreeState)

workflow.add_node("normalize",normalize_node)
workflow.add_node("initial",initial_node)
workflow.add_node("selection",selection_node)
workflow.add_node("expansion",expansion_node)
workflow.add_node("simulation",simulation_node)
workflow.add_node("evaluation",evaluation_node)
workflow.add_node("generate",generation_node)

workflow.set_entry_point("normalize")

workflow.add_edge("normalize","initial")
workflow.add_edge("initial","selection")
workflow.add_edge("selection","expansion")
workflow.add_edge("expansion","simulation")
workflow.add_edge("simulation","evaluation")

workflow.add_conditional_edges(
    "evaluation",
    should_continue,
    {
        "generate": "generate",
        "selection": "selection",
    }
)
workflow.add_edge("generate",END)
app=workflow.compile()

if __name__ == "__main__":
    # æµ‹è¯• Queryï¼šåŒ…å«ä¸€äº›éæ ‡å‡†å†™æ³• (tsmc, 3nm)
    query = "2024å¹´tsmcçš„æœ€æ–°3nmäº§èƒ½æƒ…å†µå¦‚ä½•ï¼Ÿè‹¹æœå’Œnvçš„è®¢å•å æ¯”å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
    
    print(f"ğŸš€ [Agent Start] å¯åŠ¨ MCTS Agent (èŠ¯çŸ¥ - ä¸“ä¸šåŠå¯¼ä½“ç‰ˆ)")
    print(f"â“ åŸå§‹é—®é¢˜: {query}\n" + "="*50)
    
    inputs = {
        "original_query": query, 
        "max_iterations": 6 
    }
    
    config = {"recursion_limit": 50} 
    
    try:
        final_state = app.invoke(inputs, config=config)
        
        print("\n" + "="*50)
        print("âœ… [Done] æœ€ç»ˆç­”æ¡ˆï¼š\n")
        print(final_state["best_answer"])
        print("\n" + "="*50)
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()