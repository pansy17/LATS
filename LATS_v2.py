import os
import sys

# ==============================================================================
# ğŸš¨ æ ¸å¿ƒä¿®æ­£ï¼šç¯å¢ƒå˜é‡å¿…é¡»åœ¨å¯¼å…¥ LangChain ç»„ä»¶ä¹‹å‰é…ç½®ï¼Œå¦åˆ™è¿½è¸ªå¯èƒ½å¤±æ•ˆ
# ==============================================================================
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "xxxx" 
# âš ï¸ æ³¨æ„ï¼šä¿®å¤äº†åŸä»£ç ä¸­çš„ä¸­æ–‡å¼•å·å’Œæ¢è¡Œç¬¦ï¼Œè¯·å¡«å…¥ä½ çš„çœŸå® Key
os.environ["LANGCHAIN_API_KEY"] = "xxxx" 

# ----------------- æ ‡å‡†åº“å¯¼å…¥ -----------------
import math
import ast
import operator
import re
import uuid
from typing import List, TypedDict, Optional, Dict, Any, Union

# ----------------- LangSmith å¯¼å…¥ -----------------
# ç”¨äºç»™å‡½æ•°åŠ è£…é¥°å™¨ï¼Œå®ç°ç»†ç²’åº¦è¿½è¸ª
from langsmith import traceable

# ----------------- LangChain / LangGraph å¯¼å…¥ -----------------
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableConfig 

# ----------------- ç¯å¢ƒé…ç½® -----------------
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())

# è·å–å…¶ä»– API Key
openai_api_key = os.environ.get("OPENAI_API_KEY")
openai_api_base = os.environ.get("OPENAI_API_BASE")
serper_api_key = os.environ.get("SERPER_API_KEY")

# ----------------- ç”¨æˆ·è‡ªå®šä¹‰å·¥å…·å¯¼å…¥ -----------------
try:
    from documents.retriever_tools import ensemble_retriever
    print("âœ… æˆåŠŸåŠ è½½æœ¬åœ° RAG æ£€ç´¢å™¨: ensemble_retriever")
except ImportError:
    print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ£€ç´¢å™¨ï¼Œä½¿ç”¨ Mock æ›¿ä»£")
    class MockRetriever:
        def invoke(self, query):
            return [Document(page_content=f"æ¨¡æ‹Ÿæ£€ç´¢å†…å®¹: å…³äº {query} çš„æœ¬åœ°æŠ€æœ¯æ–‡æ¡£æ•°æ®...", metadata={"source": "local"})]
    ensemble_retriever = MockRetriever()

# ----------------- å…¨å±€å·¥å…·åˆå§‹åŒ– -----------------
try:
    llm = ChatOpenAI(
        model="gpt-4o-mini", 
        temperature=0.7, 
        openai_api_key=openai_api_key, 
        openai_api_base=openai_api_base
    )
    
    web_search_tool = GoogleSerperAPIWrapper(api_key=serper_api_key)
    print("âœ… æˆåŠŸåˆå§‹åŒ– Google Serper å·¥å…·")
    
except Exception as e:
    print(f"âŒ å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
    # ä¸ºäº†æ¼”ç¤ºä¸ç›´æ¥é€€å‡ºï¼Œå®é™…ä½¿ç”¨å»ºè®®ä¿ç•™ exit(1)
    # exit(1) 


# ==========================================
# 1. MCTS æ ¸å¿ƒæ•°æ®ç»“æ„ (Node & State)
# ==========================================

class MCTSNode:
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢èŠ‚ç‚¹"""
    def __init__(self, parent=None, action_thought: str = "Root"):
        self.parent: Optional[MCTSNode] = parent
        self.children: List[MCTSNode] = []
        
        # èŠ‚ç‚¹å±æ€§
        # å½“å‰èŠ‚ç‚¹å¯¹åº”çš„â€œåŠ¨ä½œ/æ€è€ƒâ€æ–‡æœ¬ï¼Œç”¨äºæè¿°è¯¥èŠ‚ç‚¹è¦è§£å†³çš„é—®é¢˜æˆ–å­æŸ¥è¯¢
        self.action_thought = action_thought  
        # å­˜å‚¨ä»æ£€ç´¢å·¥å…·ï¼ˆRAG/Webï¼‰è¿”å›çš„åŸå§‹ä¸Šä¸‹æ–‡å†…å®¹ï¼Œä¾›åç»­ç”Ÿæˆç­”æ¡ˆä½¿ç”¨
        self.content: str = ""                
        # åŸºäº content ç”Ÿæˆçš„å¸¦å¼•ç”¨ä¸­é—´ç­”æ¡ˆï¼Œä¹Ÿæ˜¯æœ€ç»ˆç­”æ¡ˆçš„å€™é€‰ç‰‡æ®µ
        self.generated_answer: str = ""       
        
        # MCTS ç»Ÿè®¡ï¼šèŠ‚ç‚¹åœ¨æ¨¡æ‹Ÿé˜¶æ®µè¢«è®¿é—®çš„æ¬¡æ•°ï¼Œç”¨äºè®¡ç®— UCT å€¼
        self.visits: int = 0                  
        # MCTS ç»Ÿè®¡ï¼šèŠ‚ç‚¹ç´¯è®¡è·å¾—çš„å¥–åŠ±/è¯„åˆ†æ€»å’Œï¼Œç”¨äºè®¡ç®—å¹³å‡ä»·å€¼
        self.value_sum: float = 0.0           
        # èŠ‚ç‚¹åœ¨æ ‘ä¸­çš„æ·±åº¦ï¼Œæ ¹èŠ‚ç‚¹ä¸º 0ï¼Œæ¯ä¸‹é™ä¸€å±‚åŠ  1ï¼Œç”¨äºé™åˆ¶æœç´¢æ·±åº¦
        self.depth: int = 0 if parent is None else parent.depth + 1
        
    @property
    def value(self) -> float:
        if self.visits == 0:
            return 0.0
        return self.value_sum / self.visits

    def uct_score(self, parent_visits: int, c_puct: float = 1.41) -> float:
        if self.visits == 0:
            return float('inf') 
        
        q_value = self.value
        u_value = c_puct * math.sqrt(math.log(parent_visits) / (1 + self.visits))
        return q_value + u_value

    def add_child(self, child_node):
        self.children.append(child_node)

    def __repr__(self):
        return f"<Node depth={self.depth} visits={self.visits} val={self.value:.2f} thought='{self.action_thought[:10]}...'>"

class TreeState(TypedDict):
    """LangGraph çŠ¶æ€å®šä¹‰"""
    original_query: str     # ç”¨æˆ·åŸå§‹é—®é¢˜
    normalized_query: str   # å½’ä¸€åŒ–åçš„æ ‡å‡†æŸ¥è¯¢
    root: MCTSNode          # æ ‘æ ¹
    current_node: MCTSNode  # å½“å‰æ­£åœ¨å¤„ç†çš„èŠ‚ç‚¹
    iterations: int         # å½“å‰è¿­ä»£è½®æ•°
    max_iterations: int     # æœ€å¤§è¿­ä»£è½®æ•°ï¼ˆé¢„ç®—ï¼‰
    best_answer: str        # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ

# ==========================================
# 2. LATS æ ¸å¿ƒèŠ‚ç‚¹é€»è¾‘ (Nodes)
# ==========================================

@traceable(name="Normalize Query") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def normalize_node(state: TreeState):
    """ã€æŸ¥è¯¢å½’ä¸€åŒ–ã€‘"""
    query = state["original_query"]
    print(f"\nğŸ”§ [Normalize] æ­£åœ¨æ ‡å‡†åŒ–æŸ¥è¯¢: {query}")

    # 1. è§„åˆ™åº“å½’ä¸€åŒ– (Regex/Dict)
    term_mapping = {
        r"(?i)stm\s*32": "STM32",
        r"(?i)iic": "I2C",
        r"(?i)spi": "SPI",
        r"(?i)uart": "UART",
        r"(?i)mcu": "MCU",
        r"(?i)datasheet": "Data Sheet",
        r"(?i)spec\s*sheet": "Specification",
        r"(?i)tsmc": "TSMC",
        r"(?i)smic": "SMIC",
        r"(?i)nv(idia)?": "NVIDIA",
        r"(?i)3\s*nm": "3nm",
    }
    
    normalized_query = query
    for pattern, replacement in term_mapping.items():
        normalized_query = re.sub(pattern, replacement, normalized_query)

    # 2. LLM è¯­ä¹‰é‡å†™
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŠå¯¼ä½“é¢†åŸŸçš„æœ¯è¯­ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
    
    åŸå§‹æŸ¥è¯¢: "{normalized_query}"
    
    ä»»åŠ¡ï¼š
    1. **çº æ­£æ‹¼å†™**ï¼šä¿®å¤æ˜æ˜¾çš„æ‹¼å†™é”™è¯¯ã€‚
    2. **æœ¯è¯­è§„èŒƒ**ï¼šå°†éæ ‡å‡†æè¿°è½¬æ¢ä¸ºè¡Œä¸šé€šç”¨æœ¯è¯­ï¼ˆå¦‚ "3çº³ç±³å·¥è‰º" -> "3nm Process Node"ï¼‰ã€‚
    3. **å®ä½“è¡¥å…¨**ï¼šå¦‚æœå®ä½“åç§°æ¨¡ç³Šï¼Œå°è¯•è¡¥å…¨ï¼ˆå¦‚ "A17" -> "Apple A17 Pro"ï¼‰ï¼Œä½†ä¸è¦æ”¹å˜åŸæ„ã€‚
    4. **å»å£è¯­åŒ–**ï¼šå»é™¤æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼Œä¿ç•™æ ¸å¿ƒæœç´¢æ„å›¾ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ ‡å‡†åŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å¼•å·ã€‚
    """
    
    try:
        response = llm.invoke(prompt).content.strip()
        final_query = response.strip('"').strip("'")
    except Exception as e:
        print(f"âš ï¸ å½’ä¸€åŒ–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å¤„ç†ç»“æœ: {e}")
        final_query = normalized_query

    print(f"   -> æ ‡å‡†åŒ–ç»“æœ: {final_query}")
    return {"normalized_query": final_query}


def initial_node(state: TreeState):
    """ã€åˆå§‹åŒ–ã€‘åˆ›å»ºæ ¹èŠ‚ç‚¹"""
    q = state.get("normalized_query", state["original_query"])
    print(f"\nğŸŒ± [Start] åˆå§‹åŒ– LATS æ ‘æœç´¢ï¼ŒåŸºå‡†æŸ¥è¯¢: {q}")
    root = MCTSNode(action_thought=q)
    return {"root": root, "current_node": root, "iterations": 0}

@traceable(name="MCTS Selection") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def selection_node(state: TreeState):
    """ã€é€‰æ‹©ã€‘åŸºäº UCT é€‰æ‹©æœ€æœ‰æ½œåŠ›çš„å¶å­èŠ‚ç‚¹"""
    root = state["root"]
    node = root
    
    # è´ªå©ªé€‰æ‹©ç›´åˆ°å¶å­èŠ‚ç‚¹
    while node.children:
        node = max(node.children, key=lambda c: c.uct_score(node.visits))
    
    return {"current_node": node}

@traceable(name="MCTS Expansion") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def expansion_node(state: TreeState):
    """ã€æ‰©å±•ã€‘LLM ç”Ÿæˆæ–°çš„å­æŸ¥è¯¢"""
    current_node = state["current_node"]
    query = current_node.action_thought
    
    # é™åˆ¶æ·±åº¦ä¸æ‰©å±•æ¡ä»¶
    if (current_node.visits > 0 or current_node.depth == 0) and current_node.depth < 3:
        print(f"ğŸŒ² [Expand] æ­£åœ¨æ‰©å±•èŠ‚ç‚¹: '{query[:20]}...'")
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åŠå¯¼ä½“è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚é’ˆå¯¹é—®é¢˜: "{query}"
        è¯·ç”Ÿæˆ 2 åˆ° 3 ä¸ª æå…·é’ˆå¯¹æ€§çš„æœç´¢å­æŸ¥è¯¢ï¼Œæ—¨åœ¨æŒ–æ˜æ·±å±‚æ•°æ®ã€‚
        
        è¦æ±‚ï¼š
        1. **ç²¾ç¡®åŒ–åº¦é‡è¡¡**ï¼šå¦‚æœæ¶‰åŠäº§èƒ½ï¼Œå¿…é¡»åŒ…å« "WPM" (Wafers Per Month), "capacity utilization" (äº§èƒ½åˆ©ç”¨ç‡) ç­‰å…³é”®è¯ã€‚
        2. **åŒºåˆ†æ¦‚å¿µ**ï¼šæ˜ç¡®åŒºåˆ† "Revenue Share" (è¥æ”¶å æ¯”) å’Œ "Wafer Allocation" (æ™¶åœ†é…é¢/äº§èƒ½å æ¯”)ï¼Œé¿å…æ··æ·†ã€‚
        3. **å…·ä½“å·¥è‰ºèŠ‚ç‚¹**ï¼šå¯¹äºå…ˆè¿›åˆ¶ç¨‹ï¼Œå°è¯•åŠ å…¥å…·ä½“ä»£å·ï¼ˆå¦‚ TSMC N3, N3E, N3B, N3Pï¼‰ã€‚
        4. **æƒå¨æ¥æºå¯¼å‘**ï¼šå¯ä»¥åŠ ä¸Š "report", "TrendForce", "Digitimes" ç­‰å…³é”®è¯ä»¥å¼•å¯¼æœç´¢é«˜è´¨é‡ç ”æŠ¥ã€‚
        
        è¯·ä¸¥æ ¼è¿”å› Python åˆ—è¡¨æ ¼å¼å­—ç¬¦ä¸²ã€‚
        ç¤ºä¾‹: ["TSMC N3 capacity WPM 2024 forecast", "Apple vs Nvidia TSMC 3nm wafer allocation 2024"]
        """
        try:
            response = llm.invoke(prompt).content
            start = response.find('[')
            end = response.rfind(']') + 1
            sub_queries = ast.literal_eval(response[start:end])
        except Exception as e:
            print(f"âš ï¸ è§£ææ‰©å±•æŸ¥è¯¢å¤±è´¥: {e}")
            sub_queries = [f"{query} WPM details", f"{query} market share report"]

        if not sub_queries:
            sub_queries = [query]

        for q in sub_queries:
            child = MCTSNode(parent=current_node, action_thought=q)
            current_node.add_child(child)
        
        if current_node.children:
            return {"current_node": current_node.children[0]}
    
    return {"current_node": current_node}

@traceable(name="Simulation & Fact Check") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def simulation_node(state: TreeState):
    """ã€æ¨¡æ‹Ÿ - äº‹å®é”šå®šä¸æ‹’ç­”æœºåˆ¶ã€‘"""
    node = state["current_node"]
    
    if node.generated_answer:
        return {"current_node": node}
        
    query = node.action_thought
    print(f"ğŸ” [Simulate] æ‰§è¡ŒçœŸå®æ£€ç´¢: {query}")
    
    raw_docs = [] 
    
    # 1. æ‰§è¡Œ RAG æ£€ç´¢
    try:
        rag_docs = ensemble_retriever.invoke(query)
        if rag_docs:
            for d in rag_docs[:2]:
                source = d.metadata.get("source", "Local Doc")
                raw_docs.append(f"ã€æœ¬åœ°-{source}ã€‘: {d.page_content}")
    except Exception as e:
        print(f"   -> RAG é”™è¯¯: {e}")

    # 2. æ‰§è¡Œ Web Search
    try:
        web_res = web_search_tool.results(query)
        if "organic" in web_res:
            for item in web_res["organic"][:2]:
                raw_docs.append(f"ã€ç½‘ç»œ-{item.get('title')}ã€‘: {item.get('snippet')}")
    except Exception as e:
        print(f"   -> Web é”™è¯¯: {e}")

    # 3. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    if not raw_docs:
        formatted_context = "æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚"
        node.content = formatted_context
    else:
        formatted_parts = []
        for i, doc in enumerate(raw_docs):
            formatted_parts.append(f"[Ref: {i}] {doc}")
        formatted_context = "\n\n".join(formatted_parts)
        node.content = formatted_context

    # 4. ç”Ÿæˆå¸¦å¼•ç”¨çš„ä¸­é—´ç­”æ¡ˆ
    print(f"   -> æ­£åœ¨å°è¯•ç”Ÿæˆä¸­é—´ç­”æ¡ˆå¹¶è¿›è¡Œäº‹å®é”šå®š...")
    
    anchor_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠå¯¼ä½“äº§ä¸šç ”ç©¶å‘˜ã€‚è¯·åŸºäºä¸‹æ–¹çš„ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ï¼š"{query}"ã€‚

    ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘
    {formatted_context}

    ã€å›ç­”è§„åˆ™ã€‘
    1. **äº‹å®é”šå®š**ï¼šä½ ç”Ÿæˆçš„æ¯ä¸€å¥è¯ï¼Œå¦‚æœå¼•ç”¨äº†ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨æ¥æº IDï¼Œæ ¼å¼ä¸º [Ref: x]ã€‚
    2. **æ•°æ®æ•æ„Ÿæ€§**ï¼š
       - å¦‚æœæ‰¾åˆ°**å…·ä½“æ•°å­—**ï¼ˆå¦‚ "60k-70k wpm"ï¼‰ï¼Œè¯·åŠ¡å¿…ä¿ç•™å¹¶å¼•ç”¨ã€‚
       - å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯**è¥æ”¶å æ¯”**ï¼ˆRevenueï¼‰ï¼Œä¸¥ç¦å°†å…¶ç›´æ¥ç­‰åŒäº**äº§èƒ½å æ¯”**ï¼ˆCapacity/Waferï¼‰ï¼Œå¿…é¡»æ˜ç¡®æŒ‡å‡ºæ˜¯â€œè¥æ”¶â€è¿˜æ˜¯â€œäº§èƒ½â€ã€‚
    3. **é¢„æµ‹åŒ…å®¹æ€§**ï¼šå¦‚æœæ²¡æœ‰å®˜æ–¹æŠ«éœ²çš„ç²¾ç¡®æ•°æ®ï¼Œ**å…è®¸å¼•ç”¨çŸ¥ååˆ†ææœºæ„çš„ä¼°ç®—æ•°æ®**ï¼Œä½†å¿…é¡»åœ¨å›ç­”ä¸­æ˜ç¡®è¯´æ˜ã€‚
    4. **æ‹’ç­”æœºåˆ¶**ï¼šå¦‚æœä¸Šä¸‹æ–‡å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­” "æ— æ³•å›ç­”"ã€‚

    è¯·ç”Ÿæˆç²¾ç‚¼çš„å›ç­”ï¼š
    """
    
    try:
        simulation_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
        generated_answer = simulation_llm.invoke(anchor_prompt).content
    except Exception as e:
        generated_answer = "æ— æ³•å›ç­” (ç”Ÿæˆé”™è¯¯)"

    node.generated_answer = generated_answer
    print(f"   -> ä¸­é—´ç­”æ¡ˆ: {generated_answer[:50]}...")
    
    return {"current_node": node}

@traceable(name="Backpropagation Evaluation") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def evaluation_node(state: TreeState):
    """ã€è¯„ä¼° & åå‘ä¼ æ’­ã€‘"""
    node = state["current_node"]
    original_query = state["original_query"]
    answer = node.generated_answer
    
    print(f"âš–ï¸ [Evaluate] æ­£åœ¨è¯„ä¼°èŠ‚ç‚¹è´¨é‡...")
    
    score = 0.0
    
    # --- è§„åˆ™ 1: æ£€æµ‹æ‹’ç­” ---
    if "æ— æ³•å›ç­”" in answer or "No information" in answer:
        print("   -> æ£€æµ‹åˆ°æ‹’ç­”ï¼Œç»™äºˆä½åˆ†å¥–åŠ± (0.1) ä»¥é¼“åŠ±è¯šå®ã€‚")
        score = 0.1
    
    # --- è§„åˆ™ 2: æ£€æµ‹å¼•ç”¨ (Fact Anchoring Check) ---
    elif "[Ref:" in answer:
        verify_prompt = f"""
        ä½œä¸ºåŠå¯¼ä½“æ•°æ®å®¡æ ¸å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹å›ç­”è¿›è¡Œä¸¥æ ¼æ‰“åˆ† (0.0 åˆ° 1.0)ã€‚
        
        ç”¨æˆ·åŸé—®é¢˜: {original_query}
        å½“å‰å­æŸ¥è¯¢: {node.action_thought}
        ç”Ÿæˆçš„å›ç­”: {answer}
        æ£€ç´¢åˆ°çš„åŸæ–‡: 
        {node.content[:2000]}
        
        è¯„åˆ†æ ‡å‡†ï¼š
        1. **ç›¸å…³æ€§** (0.4åˆ†)ï¼šå›ç­”æ˜¯å¦ç›´æ¥è§£å†³äº†å­æŸ¥è¯¢çš„æ ¸å¿ƒé—®é¢˜ï¼Ÿ
        2. **çœŸå®æ€§** (0.4åˆ†)ï¼š[Ref: x] çš„å¼•ç”¨æ˜¯å¦çœŸå®å­˜åœ¨äºåŸæ–‡ä¸­ï¼Œä¸”æ²¡æœ‰æ­ªæ›²åŸæ„ï¼Ÿ
        3. **ç²¾å‡†åº¦** (0.2åˆ†)ï¼š
           - å¦‚æœå›ç­”åŒ…å«äº†**å…·ä½“æ•°å€¼**ï¼ˆå¦‚ "80k wpm"ï¼‰è€Œä¸æ˜¯æ¨¡ç³Šæè¿°ï¼ŒåŠ åˆ†ã€‚
           - å¦‚æœå›ç­”**æ··æ·†äº†è¥æ”¶ï¼ˆRevenueï¼‰å’Œäº§èƒ½ï¼ˆWafer/Capacityï¼‰**ï¼Œç›´æ¥æ‰£é™¤ 0.5 åˆ†ï¼ˆä¸¥é‡é”™è¯¯ï¼‰ã€‚
        
        è¯·ç»¼åˆè€ƒè™‘åï¼Œåªè¿”å›ä¸€ä¸ªæ•°å­—ã€‚
        """
        try:
            response = llm.invoke(verify_prompt).content.strip()
            match = re.search(r"0\.\d+|1\.0|0|1", response)
            if match:
                score = float(match.group())
        except:
            score = 0.5 # å‡ºé”™å›é€€
        print(f"   -> å¼•ç”¨æ ¡éªŒå¾—åˆ†: {score}")
        
    # --- è§„åˆ™ 3: æ— å¼•ç”¨ (Hallucination Risk) ---
    else:
        print("   -> âš ï¸ è­¦å‘Š: å›ç­”æœªåŒ…å«å¼•ç”¨ï¼Œåˆ¤å®šä¸ºæ½œåœ¨å¹»è§‰ï¼Œç»™äºˆ 0 åˆ†ã€‚")
        score = 0.0

    # --- Backpropagation (åå‘ä¼ æ’­) ---
    temp_node = node
    while temp_node:
        temp_node.visits += 1
        temp_node.value_sum += score
        temp_node = temp_node.parent
        
    return {"iterations": state["iterations"] + 1}

@traceable(name="Final Report Generation") # âœ… æ·»åŠ è¿½è¸ªè£…é¥°å™¨
def generation_node(state: TreeState):
    """ã€ç”Ÿæˆã€‘æ±‡æ€»é«˜ç½®ä¿¡åº¦è·¯å¾„"""
    print("\nâœï¸ [Generate] æœç´¢ç»“æŸï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    root = state["root"]
    original_query = state["original_query"]
    
    all_contexts = []
    
    def collect_contexts(node: MCTSNode):
        if node.visits > 0 and node.value > 0.4 and node.generated_answer:
            if "æ— æ³•å›ç­”" not in node.generated_answer:
                all_contexts.append(f"ã€æ¥æº: {node.action_thought}ã€‘\n{node.generated_answer}")
        
        for child in node.children:
            collect_contexts(child)
            
    collect_contexts(root)
    
    unique_contexts = "\n\n".join(list(set(all_contexts))[:10])
    
    if not unique_contexts:
        unique_contexts = "æœªæ£€ç´¢åˆ°è¶³å¤Ÿçš„å¯ä¿¡ä¿¡æ¯ã€‚"

    final_prompt = f"""
    ä½ æ˜¯ç”±å­—èŠ‚è·³åŠ¨ RAG æŠ€æœ¯æ”¯æŒçš„åŠå¯¼ä½“é¦–å¸­æˆ˜ç•¥é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹**ç»è¿‡äº‹å®æ ¸æŸ¥**çš„ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½å…³äº "{original_query}" çš„ç®€æŠ¥ã€‚
    
    === ç»è¿‡æ ¸æŸ¥çš„æƒ…æŠ¥ç¢ç‰‡ ===
    {unique_contexts}
    =========================
    
    æ’°å†™è¦æ±‚ï¼š
    1. **ç»“æ„åŒ–è¾“å‡º**ï¼šè¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«ã€æ ¸å¿ƒç»“è®ºã€‘ã€ã€æ•°æ®è¯¦è§£ã€‘ã€ã€é£é™©/ä¸ç¡®å®šæ€§æç¤ºã€‘ä¸‰ä¸ªéƒ¨åˆ†ã€‚
    2. **æ•°æ®ç²¾å‡†**ï¼šä¼˜å…ˆå±•ç¤ºå…·ä½“æ•°å­—ï¼ˆå¦‚äº§èƒ½ WPMã€è‰¯ç‡ %ï¼‰ã€‚å¦‚æœå¼•ç”¨çš„æ˜¯åˆ†ææœºæ„ï¼ˆå¦‚ TrendForceï¼‰çš„ä¼°ç®—å€¼ï¼Œè¯·æ˜ç¡®æ ‡æ³¨â€œä¼°ç®—â€ã€‚
    3. **æ¦‚å¿µå˜æ¸…**ï¼šåœ¨æè¿°å æ¯”æ—¶ï¼Œæ˜ç¡®åŒºåˆ†æ˜¯â€œè¥æ”¶è´¡çŒ®å æ¯”â€è¿˜æ˜¯â€œæ™¶åœ†äº§èƒ½å æ¯”â€ï¼Œè‹¥æ•°æ®ç¼ºå¤±è¯·è¯´æ˜ã€‚
    4. **æ¥æºæ ‡æ³¨**ï¼šåœ¨å…³é”®æ•°æ®åä¿ç•™ [Ref] æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œæˆ–è¯´æ˜æ¥æºäºå“ªä¸ªå­æŸ¥è¯¢ã€‚
    5. **å»ä¼ªå­˜çœŸ**ï¼šå¦‚æœç¢ç‰‡ä¿¡æ¯ä¸­å­˜åœ¨å†²çªï¼Œè¯·å¯¹æ¯”å±•ç¤ºï¼Œä¸è¦å¼ºè¡Œåˆå¹¶ã€‚
    
    æœ€ç»ˆç®€æŠ¥ï¼š
    """
    
    final_answer = llm.invoke(final_prompt).content
    return {"best_answer": final_answer}

# ==========================================
# 3. è·¯ç”±é€»è¾‘
# ==========================================

def should_continue(state: TreeState):
    if state["iterations"] < state["max_iterations"]:
        return "selection"
    return "generate"

# ==========================================
# 4. æ„å»º LangGraph
# ==========================================

workflow = StateGraph(TreeState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("normalize", normalize_node) 
workflow.add_node("initial", initial_node)
workflow.add_node("selection", selection_node)
workflow.add_node("expansion", expansion_node)
workflow.add_node("simulation", simulation_node)
workflow.add_node("evaluation", evaluation_node)
workflow.add_node("generate", generation_node)

# è®¾ç½®è¾¹
workflow.set_entry_point("normalize")
workflow.add_edge("normalize", "initial")
workflow.add_edge("initial", "selection")
workflow.add_edge("selection", "expansion")
workflow.add_edge("expansion", "simulation")
workflow.add_edge("simulation", "evaluation")

workflow.add_conditional_edges(
    "evaluation",
    should_continue,
    {
        "selection": "selection",
        "generate": "generate"
    }
)

workflow.add_edge("generate", END)
app = workflow.compile()

# ==========================================
# 5. æ‰§è¡Œ (å¸¦è¿½è¸ª Metadata)
# ==========================================

if __name__ == "__main__":
    # æµ‹è¯• Query
    query = "2024å¹´tsmcçš„æœ€æ–°3nmäº§èƒ½æƒ…å†µå¦‚ä½•ï¼Ÿè‹¹æœå’Œnvçš„è®¢å•å æ¯”å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
    
    # ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ run_id ä»¥ä¾¿åœ¨ LangSmith ä¸­é€šè¿‡ ID æœç´¢ï¼ˆå¯é€‰ï¼‰
    run_id = str(uuid.uuid4())
    
    print(f"ğŸš€ [Agent Start] å¯åŠ¨ MCTS Agent (èŠ¯çŸ¥ - ä¸“ä¸šåŠå¯¼ä½“ç‰ˆ)")
    print(f"ğŸ“¡ LangSmith Trace ID: {run_id}")
    print(f"â“ åŸå§‹é—®é¢˜: {query}\n" + "="*50)
    
    inputs = {
        "original_query": query, 
        "max_iterations": 6 
    }
    
    # ===ã€æ ¸å¿ƒé…ç½®ã€‘é€šè¿‡ config ä¼ é€’è¿½è¸ªä¿¡æ¯ ===
    config = {
        "recursion_limit": 50,
        "run_name": "MCTS_Semiconductor_Run", # è¿™å°†åœ¨ LangSmith åˆ—è¡¨ä¸­æ˜¾ç¤º
        "metadata": {
            "user_id": "test_user_001",
            "environment": "development",
            "algo_type": "MCTS-LATS"
        },
        "configurable": {"thread_id": run_id}
    }
    
    try:
        final_state = app.invoke(inputs, config=config)
        
        print("\n" + "="*50)
        print("âœ… [Done] æœ€ç»ˆç­”æ¡ˆï¼š\n")
        print(final_state["best_answer"])
        print("\n" + "="*50)
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()